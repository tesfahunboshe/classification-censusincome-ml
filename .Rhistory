summary_binary(prediction_bagging)
length(prediction_bagging)
prediction_bagging <-
sapply(results_bagging,
function(x)
predict(object = x,
newdata = data_test[selectedvars[!selectedvars %in% "PTOTVAL"]], # exclude the target variable
type = "class")) %>%
data.frame()
prediction_bagging <- (prediction_bagging=="YES")*1
length(prediction_bagging)
nrow(length(prediction_bagging))
nrow(prediction_bagging))
nrow(prediction_bagging)
prediction_bagging
confusionMatrix(data = prediction_bagging,
reference =data_test$PTOTVAL,
positive = "YES")
prediction_bagging_YESNO <- ifelse(rowSums(prediction_bagging) > 7.5,
"NO", "YES") %>%
factor(., levels = c("YES", "NO"))
confusionMatrix(data = prediction_bagging,
reference =data_test$PTOTVAL,
positive = "YES")
prediction_bagging_YESNO <- ifelse(rowSums(prediction_bagging) > 7.5,
"NO", "YES") %>%
factor(., levels = c("YES", "NO"))
confusionMatrix(data = prediction_bagging_YESNO,
reference =data_test$PTOTVAL,
positive = "YES")
prediction_bagging_YESNO <- ifelse(rowSums(prediction_bagging) >= 7.5,
"NO", "YES") %>%
factor(., levels = c("YES", "NO"))
metrics_IG_final <- confusionMatrix(data = ifelse(rowSums(prediction_test_IG_final[,2]) > 0.5,"NO", "YES"),
reference =data_test$PTOTVAL,
positive = "YES")
metrics_IG_final <- confusionMatrix(data = ifelse(prediction_test_IG_final[,2]> 0.5,"NO", "YES"),
reference =data_test$PTOTVAL,
positive = "YES")
data_test$PTOTVAL
data = ifelse(prediction_test_IG_final[,2]> 0.5,"NO", "YES")
data
levels(data = ifelse(prediction_test_IG_final[,2]> 0.5,"NO", "YES"))
levels(data)
as.factor(ifelse(prediction_test_IG_final[,2]> 0.5,"NO", "YES"))
metrics_IG_final <- confusionMatrix(data = as.factor(ifelse(prediction_test_IG_final[,2]> 0.5,"NO", "YES")),
reference =data_test$PTOTVAL,
positive = "YES")
metrics_bagging <- confusionMatrix(data = prediction_bagging_YESNO,
reference =data_test$PTOTVAL,
positive = "YES")
metrics_bagging
metrics_IG_final
cbind(metrics_IG_final,metrics_IG_final)
(metrics_IG_final <- confusionMatrix(data = as.factor(ifelse(prediction_test_IG_final[,2]> 0.5,"NO", "YES")),
reference =data_test$PTOTVAL,
positive = "YES"))
(metrics_bagging <- confusionMatrix(data = prediction_bagging_YESNO,
reference =data_test$PTOTVAL,
positive = "YES"))
(metrics_IG_final <- confusionMatrix(data = as.factor(ifelse(prediction_test_IG_final[,2]> 0.5,"NO", "YES")),
reference =data_test$PTOTVAL,
positive = "YES"))
datausa.ROC.test.bag <-
roc(as.numeric(data_test$PTOTVAL == "Yes"),
prediction_bagging)
prediction_bagging
datausa.ROC.test.bag <-
roc(as.numeric(data_test$PTOTVAL == "Yes"),
rowMeans(prediction_bagging))
rowMeans(prediction_bagging)
class(rowMeans(prediction_bagging))
levels(rowMeans(prediction_bagging))
levels(prediction_bagging)
prediction_bagging
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
requiredPackages = c("e1071","randomForest","janitor","verification","olsrr","DescTools","caret","tibble","purrr","corrplot","dbplyr","dplyr","readr", "ggplot2", "tidyverse","tree","rpart","rpart.plot","rattle","here", "pROC","xgboost","neuralnet")
for(i in requiredPackages){if(!require(i,character.only = TRUE)) install.packages(i)}
for(i in requiredPackages){if(!require(i,character.only = TRUE)) library(i,character.only = TRUE)}
setwd("C:\\Users\\Tesfahun Boshe\\Documents\\classification-censusincome-ml")
data_train <- read.csv("CensusData_Train.csv")
data_test <- read.csv("CensusData_Test.csv")
summary(data_train)
table(data_train$PTOTVAL)
# is.factor(data_train$PTOTVAL)
data_train$PTOTVAL <- factor(data_train$PTOTVAL,
levels = c(" 50000+.","-50000"), #level 5000 is set as the first one
ordered = TRUE) # ordinal
# levels(data_train$PTOTVAL)
table(data_train$PTOTVAL)
sum(is.na(data_train))
sapply(data_train, function(x) sum(is.na(x)))
sapply(data_train, function(x) sum(x==" ?"))
data_train <- data_train[!data_train$PENATVTY == " ?",]
categorical_vars <-
sapply(data_train, is.character) %>%
which() %>%
names()
categorical_vars
data_train[categorical_vars] <- lapply(data_train[categorical_vars],factor)
lapply(data_train[categorical_vars],levels)
options(contrasts = c("contr.treatment",  # for non-ordinal factors
"contr.treatment")) # for ordinal factors
numeric_vars <-
sapply(data_train, is.numeric) %>%
which() %>%
names()
numeric_vars
sapply(data_train[, numeric_vars],
function(x)
unique(x) %>%
length()) %>%
sort()
correlations <- cor(data_train[, numeric_vars],
use = "pairwise.complete.obs")
# using the 30 most correlated variables
corrplot.mixed(correlations,
upper = "number",
lower = "circle",
tl.col = "black",
tl.pos = "lt")
( findLinearCombos(data_train[,numeric_vars] ) ->
linearCombos )
data_train$PTOTVAL <- ifelse(data_train$PTOTVAL=="-50000",0,1)
categorical_vars_all <- c(categorical_vars,"PTOTVAL") # add the dependent variable to the catergorical variables list.
model <- lm(PTOTVAL ~ .,
data = data_train[categorical_vars_all])
# summary(model)
ols_step_backward_p(model,
prem = 0.05,
# show progress
progress = FALSE) -> backward_selector
backward_selector$removed
categorical_vars_all <- categorical_vars_all[!categorical_vars_all %in% backward_selector$removed]
selectedvars <- c(categorical_vars_all ,numeric_vars)
data_train$PTOTVAL <- as.factor(ifelse(data_train$PTOTVAL==1,"YES","NO"))
data_test <- read.csv("CensusData_Test.csv")
data_test <- data_test[!data_test$PENATVTY == " ?",]
data_test[categorical_vars] <- lapply(data_test[categorical_vars],factor)
data_test$PTOTVAL <- as.factor(ifelse(data_test$PTOTVAL=="-50000","NO","YES"))
tc <- trainControl(method = "cv",
number = 10,
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = "final")
set.seed(234356) # seeding for reproducibility
model_GINI <- rpart(PTOTVAL ~ ., # model formula
data = data_train[selectedvars], # data
method = "class", # type of the tree: classification
parms = list(split = 'gini'))  # gini for decision criterion
prediction_train_GINI <- predict(model_GINI,
data_train[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")
prediction_test_GINI <- predict(model_GINI,
data_test[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")
model_GINI_final <- rpart(PTOTVAL ~ ., # model formula
data = data_train[selectedvars], # data
method = "class", # type of the tree: classification
parms = list(split = 'gini'),  # gini for decision criterion
minsplit = 500, # chosen after multiple attempts
minbucket = 250, # chosen after multiple attempts
maxdepth = 30, # chosen after multiple attempts
cp = -1
)
prediction_train_GINI_final <- predict(model_GINI_final,
data_train[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")
prediction_test_GINI_final <- predict(model_GINI_final,
data_test[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")
model_IG <- rpart(PTOTVAL ~ ., # model formula
data = data_train[selectedvars], # data
method = "class", # type of the tree: classification
parms = list(split = 'information'))  # entropy for decision criterion
prediction_train_IG <- predict(model_GINI_final,
data_train[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")
prediction_test_IG <- predict(model_GINI_final,
data_test[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")
model_IG_final <- rpart(PTOTVAL ~ ., # model formula
data = data_train[selectedvars], # data
method = "class", # type of the tree: classification
parms = list(split = 'information'),  # entropy for decision criterion
minsplit = 500, # chosen after multiple attempts
minbucket = 250,# chosen after multiple attempts
maxdepth = 30, # chosen after multiple attempts
cp = -1 )
prediction_train_IG_final <- predict(model_GINI_final,
data_train[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")
prediction_test_IG_final <- predict(model_GINI_final,
data_test[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")
fancyRpartPlot(model_GINI)
fancyRpartPlot(model_GINI_final)
fancyRpartPlot(model_IG)
fancyRpartPlot(model_IG_final)
set.seed(12345)
model_RF <- randomForest(PTOTVAL ~ .,
data = data_train[selectedvars])
data_test <- rbind(data_train[1, ] , data_test)
data_test <- data_test[-1,]
prediction_train_RF <- predict(model_RF,
data_train[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")
prediction_test_RF <- predict(model_RF,
data_test[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")
summary_binary <- function(predicted_probs,
real = data_test$PTOTVAL,
cutoff = 0.5,
level_positive = "NO",
level_negative = "YES") {
ctable <- confusionMatrix(as.factor(ifelse(predicted_probs >= cutoff,
level_positive,
level_negative)),
real,
level_positive)
stats <- round(c(ctable$overall[1],
ctable$byClass[c(1:4, 7, 11)]),5)
return(stats)
}
predictions <- c(prediction_test_GINI_final[,2], prediction_test_IG_final[,2], prediction_test_RF[,2])
# statistics_by_model <- lapply(predictions, summary_binary)
summary_binary(prediction_test_RF[,2])
summary_binary(prediction_test_IG_final[,2])
summary_binary(prediction_test_RF[,2])
n <- nrow(data_train[selectedvars])
results_bagging <- list() # collect resulting models
for (sample in 1:15) {
# we draw n-element sample (with replacement)
set.seed(12345 + sample)
data_sample <-
data_train[selectedvars][sample(x = 1:n,
size = n,
replace = TRUE),]
results_bagging[[sample]] <-  rpart(PTOTVAL ~ ., # model formula
data = data_train[selectedvars], # data
method = "class", # type of the tree: classification
parms = list(split = 'information'),  # entropy for decision criterion
minsplit = 500, # chosen after multiple attempts
minbucket = 250,# chosen after multiple attempts
maxdepth = 30, # chosen after multiple attempts
cp = -1 )
rm(data_sample)
}
prediction_bagging <-
sapply(results_bagging,
function(x)
predict(object = x,
newdata = data_test[selectedvars[!selectedvars %in% "PTOTVAL"]], # exclude the target variable
type = "class")) %>%
data.frame()
prediction_bagging <- (prediction_bagging=="YES")*1
prediction_bagging_YESNO <- ifelse(rowSums(prediction_bagging) > 7.5,
"NO", "YES") %>%
factor(., levels = c("YES", "NO"))
(metrics_IG_final <- confusionMatrix(data = as.factor(ifelse(prediction_test_IG_final[,2]> 0.5,"NO", "YES")),
reference =data_test$PTOTVAL,
positive = "YES"))
(metrics_bagging <- confusionMatrix(data = prediction_bagging_YESNO,
reference =data_test$PTOTVAL,
positive = "YES"))
# datausa.ROC.test.bag <-
#   roc(as.numeric(data_test$PTOTVAL == "Yes"),
#       rowMeans(prediction_bagging))
# parameter tuning
# parameters_gbm <- expand.grid(interaction.depth = c(1, 2, 4),
#                              n.trees = c(100, 500),
#                              shrinkage = c(0.01, 0.1),
#                              n.minobsinnode = c(100, 250, 500))
# tc2 <- trainControl(method = "cv",
#                          number = 3,
#                          classProbs = TRUE,
#                          summaryFunction = twoClassSummary,
#                          savePredictions = "final")
# set.seed(123456)
# model_GBM  <- train(PTOTVAL ~ .,
#                        data = data_train[selectedvars][1:1000,], # choosing the first 1000 rows from the training dataset
#                        distribution = "bernoulli",
#                        method = "gbm",
#                        # tuneGrid = parameters_gbm,
#                        # trControl = tc2,
#                        verbose = FALSE)
#
# # best set of parameters
# model_GBM
# Hence:
#
# n.trees = 500,
# interaction.depth = 4,
# shrinkage = 0.1
# n.minobsinnode = 100
# prediction_train_GBM <- predict(model_GBM,
#                             data_train[selectedvars[!selectedvars %in% "PTOTVAL"]][1:1000,])
#
# prediction_test_GBM <- predict(model_GBM,
#                             data_test[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")
tc2 <- trainControl(method = "cv",
number = 3,
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = "final")
# testing for various values of min_child_weight
parameters_grid <- expand.grid(nrounds = 80,
max_depth = seq(5, 15, 2),
eta = c(0.25),
gamma = 1,
colsample_bytree = c(0.2),
min_child_weight = seq(200, 1000, 200),
subsample = 0.8)
set.seed(123456789)
model_1 <- train(PTOTVAL ~ .,
data = data_train[selectedvars],
method = "xgbTree",
trControl = tc2,
tuneGrid  = parameters_grid)
model_1
# testing for various values of colsample_bytree
parameters_grid2 <- expand.grid(nrounds = 80,
max_depth = 9,
eta = c(0.25),
gamma = 1,
colsample_bytree = seq(0.1, 0.8, 0.1),
min_child_weight = 200,
subsample = 0.8)
set.seed(123456789)
model_2 <- train(PTOTVAL ~ .,
data = data_train[selectedvars],
method = "xgbTree",
trControl = tc2,
tuneGrid  = parameters_grid2)
model_2
# testing for various values of optimal length of the subsample
parameters_grid3 <- expand.grid(nrounds = 80,
max_depth = 9,
eta = c(0.25),
gamma = 1,
colsample_bytree = 0.7,
min_child_weight = 200,
subsample = c(0.6, 0.7, 0.75, 0.8, 0.85, 0.9))
set.seed(123456789)
model_3 <- train(PTOTVAL ~ .,
data = data_train[selectedvars],
method = "xgbTree",
trControl = tc2,
tuneGrid  = parameters_grid3)
model_3
# lower the learning rate and proportionally increase number of trees
paramters_grid4 <- expand.grid(nrounds = 160, # number of trees.
max_depth = 9,
eta = 0.12,  # learnign rate
gamma = 1,
colsample_bytree = 0.7,
min_child_weight = 200,
subsample = 0.9)
set.seed(2234)
model_XGB <- train(PTOTVAL ~ .,
data = data_train[selectedvars],
method = "xgbTree",
trControl = tc2,
tuneGrid  = paramters_grid4)
model_XGB
prediction_train_XGB <- predict(model_XGB,
data_train[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")
prediction_test_XGB <- predict(model_XGB,
data_test[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")
summary_binary(prediction_test_XGB[,2])
View(prediction_test_XGB)
models <- c("model_IG_final", "model_RF","model_XGB")
predictions_all <- as.data.frame(IG = prediction_train_IG_final,
RF = prediction_train_RF,
XG = prediction_train_XGB)
prediction_train_IG_final
predictions_all <- as.data.frame(IG = prediction_train_IG_final[,2],
RF = prediction_train_RF[,2],
XG = prediction_train_XGB[,2])
prediction_train_RF[,2]
prediction_train_XGB[,2]
RF = prediction_train_RF[,2]
XG = prediction_train_XGB[,2]
IG = prediction_train_IG_final[,2]
data.frame(RF,XG,IG)
predictions_all <- data.frame(IG = prediction_train_IG_final[,2],
RF = prediction_train_RF[,2],
XG = prediction_train_XGB[,2])
corrplot::corrplot(cor(predictions_all))
# Blending with caretEnsemble
model_ensemble <- caretEnsemble(
models,
metric = "ROC", # criteria of selection
trControl = trainControl(
number = 2,
summaryFunction = twoClassSummary,
classProbs = TRUE,
savePredictions = "final" # saves the best predictions
))
summary(model_ensemble)
# Blending with caretEnsemble
model_ensemble <- caretEnsemble(
models,
metric = "ROC", # criteria of selection
trControl = trainControl(
number = 2,
summaryFunction = twoClassSummary,
classProbs = TRUE,
savePredictions = "final" # saves the best predictions
))
?caretEnsemble
library(caret)
# Blending with caretEnsemble
model_ensemble <- caretEnsemble(
models,
metric = "ROC", # criteria of selection
trControl = trainControl(
number = 2,
summaryFunction = twoClassSummary,
classProbs = TRUE,
savePredictions = "final" # saves the best predictions
))
library(caretEnsemble)
install.packages("caretEnsemble")
library(caretEnsemble)
# Blending with caretEnsemble
model_ensemble <- caretEnsemble(
models,
metric = "ROC", # criteria of selection
trControl = trainControl(
number = 2,
summaryFunction = twoClassSummary,
classProbs = TRUE,
savePredictions = "final" # saves the best predictions
))
models <- c(model_IG_final, model_RF, model_XGB)
# Blending with caretEnsemble
model_ensemble <- caretEnsemble(
models,
metric = "ROC", # criteria of selection
trControl = trainControl(
number = 1,
summaryFunction = twoClassSummary,
classProbs = TRUE,
savePredictions = "final" # saves the best predictions
))
?caretList
head(predictions_all)
predictions_all$Ensemble <- rowMeans(predictions_all)
head(predictions_all)
summary_binary(predictions_all$Ensemble)
nrow(predictions_all$Ensemble)
length(predictions_all$Ensemble)
nrow(predictions_all)
nrow(prediction_train_IG_final[,2])
length(prediction_train_IG_final[,2])
?train
head(trainSet)
head(train_set)
head(data_train$PTOTVAL)
train_set <- predictions_all
train_set$outcome <- data_train$PTOTVAL
model_glm<-
train(trainSet[,1:3],trainSet[,4],method='glm')
model_glm<-
train(train_set[,1:3],train_set[,4],method='glm')
predictions_test_all <- data.frame(IG = prediction_test_IG_final[,2],
RF = prediction_test_RF[,2],
XG = prediction_test_XGB[,2])
predictions_test_all$Ensemble <- predict(model_glm,predictions_test_all)
head(predictions_test_all)
summary_binary(predictions_all$Ensemble)
summary_binary(predictions_test_all$Ensemble)
data_train_numberic <- data.frame(lapply(data_train,as.numeric))
data_test_numberic <- lapply(data_test,as.numeric)
max_points <- apply(data_train_numberic, 2, max)
min_points <- apply(data_train_numberic, 2, min)
data_train.scaled <-
as.data.frame(scale(data_train_numberic,
center = min_points,
scale  = max_points - min_points))
data_test.scaled <-
as.data.frame(scale(data_test_numberic,
center = min_points,
scale  = max_points - min_points))
max_points
min_points
data_test.scaled <-
as.data.frame(scale(data_test_numberic,
center = min_points,
scale  = max_points - min_points))
length(min_points)
data_test_numberic <- data.frame(lapply(data_test,as.numeric))
data_test.scaled <-
as.data.frame(scale(data_test_numberic,
center = min_points,
scale  = max_points - min_points))
colnames(data_train)
1:!
1:1
# training the model
model_NN <- neuralnet(PTOTVAL ~ .,
data = data_train.scaled,
hidden = c(5),
linear.output = T)
?performance
?roc
roc(data_test$PTOTVAL, prediction_test_IG)
prediction_test_IG
roc(as.numeric(data_test$PTOTVAL), ifelse(prediction_test_IG[,2]>0.5,"NO","YES"))
roc(as.numeric(data_test$PTOTVAL), as.numeric(ifelse(prediction_test_IG[,2]>0.5,"NO","YES")))
roc(as.numeric(data_test$PTOTVAL), as.numeric(as.factor(ifelse(prediction_test_IG[,2]>0.5,"NO","YES"))))
?lapply
prob_2_class <- function(data)
{
return(ifelse(data>0.5,"YES","NO"))
}
lapply(c(prediction_test_GINI_final[,2], prediction_test_IG_final[,2], prediction_test_RF[,2], prediction_test_XGB[,2]),prob_2_class)
lapply(lapply(c(prediction_test_GINI_final[,2], prediction_test_IG_final[,2], prediction_test_RF[,2], prediction_test_XGB[,2]),prob_2_class),roc(data_test$PTOTVAL))
roc(data_test$PTOTVAL,prediction_test_RF[,2])
