theme_bw() + coord_fixed() +
scale_color_brewer(palette = "Set2")
(AUC_all <- lapply(ROC_data, auc))
# training the model
model_NN <- neuralnet(PTOTVAL ~ .,
data = data_train.scaled,
hidden = c(1),
linear.output = T)
# calculating predictions
prediction_test_NN <- compute(model_NN, data_test.scaled[, 1:20])
# rescaling
prediction_test_NN <-
prediction_test_NN$net.result *
(max_points["PTOTVAL"] - min_points["PTOTVAL"]) + min_points["PTOTVAL"]
ctable <- confusionMatrix(as.factor(ifelse(round(prediction_test_NN)==1,"NO","YES")),
data_test$PTOTVAL,
"YES")
stats_NN <- round(c(ctable$overall[1],
ctable$byClass[c(1:4, 7, 11)]),5)
(stats_ALL <- rbind(stats_ALL,stats_NN))
prob_2_class <- function(data)
{
return(as.numeric(ifelse(data>0.5,1,0)))
}
predictionss <- data.frame(DT_GINI = prob_2_class(prediction_test_GINI_final[,2]),
DT_IG = prob_2_class(prediction_test_IG_final[,2]),
RF = prob_2_class(prediction_test_RF[,2]),
XGB = prob_2_class(prediction_test_XGB[,2]),
Ensemble = prob_2_class(predictions_test_Ensemble),
NN = ifelse(round(prediction_test_NN)==1,0,1))
ROC_data<- lapply(predictionss, roc, response = as.numeric(data_test$PTOTVAL == "YES"))
# roc(prob_2_class(prediction_test_GINI_final[,2]),as.numeric(data_test$PTOTVAL == "YES"))
ROC_data %>%
pROC::ggroc(alpha = 0.5, linetype = 1, size = 1) +
geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1),
color = "grey",
linetype = "dashed") +
labs(title = "Comparison Using ROCs") +
theme_bw() + coord_fixed() +
scale_color_brewer(palette = "Set2")
(AUC_all <- lapply(ROC_data, auc))
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
requiredPackages = c("e1071","randomForest","caretEnsemble","verification","olsrr","DescTools","caret","tibble","purrr","corrplot","dbplyr","dplyr","readr", "ggplot2", "tidyverse","tree","rpart","rpart.plot","rattle","here", "pROC","xgboost","neuralnet")
for(i in requiredPackages){if(!require(i,character.only = TRUE)) install.packages(i)}
for(i in requiredPackages){if(!require(i,character.only = TRUE)) library(i,character.only = TRUE)}
setwd("C:\\Users\\Tesfahun Boshe\\Documents\\classification-censusincome-ml")
data_train <- read.csv("CensusData_Train.csv")
data_test <- read.csv("CensusData_Test.csv")
str(data_train)
table(data_train$PTOTVAL)
# is.factor(data_train$PTOTVAL)
data_train$PTOTVAL <- factor(data_train$PTOTVAL,
levels = c(" 50000+.","-50000"), #level 5000 is set as the first one
ordered = TRUE) # ordinal
table(data_train$PTOTVAL)
sum(is.na(data_train))
sapply(data_train, function(x) sum(is.na(x)))
sapply(data_train, function(x) sum(x==" ?"))
data_train <- data_train[!data_train$PENATVTY == " ?",]
categorical_vars <-
sapply(data_train, is.character) %>%
which() %>%
names()
categorical_vars
data_train[categorical_vars] <- lapply(data_train[categorical_vars],factor)
lapply(data_train[categorical_vars],levels)
options(contrasts = c("contr.treatment",  # for non-ordinal factors
"contr.treatment")) # for ordinal factors
numeric_vars <-
sapply(data_train, is.numeric) %>%
which() %>%
names()
numeric_vars
sapply(data_train[, numeric_vars],
function(x)
unique(x) %>%
length()) %>%
sort()
correlations <- cor(data_train[, numeric_vars],
use = "pairwise.complete.obs")
# using the 30 most correlated variables
corrplot.mixed(correlations,
upper = "number",
lower = "circle",
tl.col = "black",
tl.pos = "lt")
( findLinearCombos(data_train[,numeric_vars] ) ->
linearCombos )
data_train$PTOTVAL <- ifelse(data_train$PTOTVAL=="-50000",0,1)
categorical_vars_all <- c(categorical_vars,"PTOTVAL") # add the dependent variable to the catergorical variables list.
model <- lm(PTOTVAL ~ .,
data = data_train[categorical_vars_all])
# summary(model)
ols_step_backward_p(model,
prem = 0.05,
# show progress
progress = FALSE) -> backward_selector
backward_selector$removed
categorical_vars_all <- categorical_vars_all[!categorical_vars_all %in% backward_selector$removed]
selectedvars <- c(categorical_vars_all ,numeric_vars)
data_train$PTOTVAL <- as.factor(ifelse(data_train$PTOTVAL==1,"YES","NO"))
data_test <- read.csv("CensusData_Test.csv")
data_test <- data_test[!data_test$PENATVTY == " ?",]
data_test[categorical_vars] <- lapply(data_test[categorical_vars],factor)
data_test$PTOTVAL <- as.factor(ifelse(data_test$PTOTVAL=="-50000","NO","YES"))
set.seed(234356) # seeding for reproducibility
model_GINI <- rpart(PTOTVAL ~ ., # model formula
data = data_train[selectedvars], # data
method = "class", # type of the tree: classification
parms = list(split = 'gini'))  # gini for decision criterion
prediction_train_GINI <- predict(model_GINI,
data_train[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")
prediction_test_GINI <- predict(model_GINI,
data_test[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")
model_GINI_final <- rpart(PTOTVAL ~ ., # model formula
data = data_train[selectedvars], # data
method = "class", # type of the tree: classification
parms = list(split = 'gini'),  # gini for decision criterion
minsplit = 800, # chosen after multiple attempts
minbucket = 400, # chosen after multiple attempts
maxdepth = 20, # chosen after multiple attempts
cp = -1
)
prediction_train_GINI_final <- predict(model_GINI_final,
data_train[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")
prediction_test_GINI_final <- predict(model_GINI_final,
data_test[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")
model_IG <- rpart(PTOTVAL ~ ., # model formula
data = data_train[selectedvars], # data
method = "class", # type of the tree: classification
parms = list(split = 'information'))  # entropy for decision criterion
prediction_train_IG <- predict(model_GINI_final,
data_train[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")
prediction_test_IG <- predict(model_GINI_final,
data_test[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")
model_IG_final <- rpart(PTOTVAL ~ ., # model formula
data = data_train[selectedvars], # data
method = "class", # type of the tree: classification
parms = list(split = 'information'),  # entropy for decision criterion
minsplit = 800, # chosen after multiple attempts
minbucket = 400,# chosen after multiple attempts
maxdepth = 20, # chosen after multiple attempts
cp = -1 )
prediction_train_IG_final <- predict(model_GINI_final,
data_train[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")
prediction_test_IG_final <- predict(model_GINI_final,
data_test[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")
fancyRpartPlot(model_GINI)
fancyRpartPlot(model_GINI_final)
fancyRpartPlot(model_IG)
fancyRpartPlot(model_IG_final)
set.seed(12345)
model_RF <- randomForest(PTOTVAL ~ .,
data = data_train[selectedvars])
data_test <- rbind(data_train[1, ] , data_test)
data_test <- data_test[-1,]
prediction_train_RF <- predict(model_RF,
data_train[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")
prediction_test_RF <- predict(model_RF,
data_test[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")
summary_binary <- function(predicted_probs,
real = data_test$PTOTVAL,
binary = TRUE,
cutoff = 0.5,
level_positive = "YES",
level_negative = "NO") {
if(binary){
ctable <- confusionMatrix(as.factor(ifelse(predicted_probs >= cutoff,
level_positive,
level_negative)),
real,
level_positive)
}
if(!binary){
ctable <- confusionMatrix(as.factor(predicted_probs),
real,
level_positive)
}
stats <- round(c(ctable$overall[1],
ctable$byClass[c(1:4, 7, 11)]),5)
return(stats)
}
# predictions <- c(prediction_test_GINI_final[,2], prediction_test_IG_final[,2], prediction_test_RF[,2])
# statistics_by_model <- lapply(predictions, summary_binary)
stats_GINI <- summary_binary(prediction_test_GINI_final[,2])
stats_IG <- summary_binary(prediction_test_IG_final[,2])
stats_RF <- summary_binary(prediction_test_RF[,2])
(stats_ALL <- rbind(stats_GINI,stats_IG,stats_RF))
n <- nrow(data_train[selectedvars])
results_bagging <- list() # collect resulting models
for (sample in 1:15) {
# we draw n-element sample (with replacement)
set.seed(12345 + sample)
data_sample <-
data_train[selectedvars][sample(x = 1:n,
size = n,
replace = TRUE),]
results_bagging[[sample]] <-  rpart(PTOTVAL ~ ., # model formula
data = data_train[selectedvars], # data
method = "class", # type of the tree: classification
parms = list(split = 'information'),  # entropy for decision criterion
minsplit = 800, # chosen after multiple attempts
minbucket = 400,# chosen after multiple attempts
maxdepth = 20, # chosen after multiple attempts
cp = -1 )
rm(data_sample)
}
prediction_bagging <-
sapply(results_bagging,
function(x)
predict(object = x,
newdata = data_test[selectedvars[!selectedvars %in% "PTOTVAL"]], # exclude the target variable
type = "class")) %>%
data.frame()
prediction_bagging <- (prediction_bagging=="YES")*1
prediction_bagging_YESNO <- ifelse(rowSums(prediction_bagging) > 7.5,
"YES", "NO") %>%
factor(., levels = c("YES", "NO"))
(metrics_IG_final <- confusionMatrix(data = as.factor(ifelse(prediction_test_IG_final[,2]> 0.5,"YES", "NO")),
reference =data_test$PTOTVAL,
positive = "YES"))
(metrics_bagging <- confusionMatrix(data = prediction_bagging_YESNO,
reference =data_test$PTOTVAL,
positive = "YES"))
control <- trainControl(method = "cv", # using cross validation
number = 3,
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = "final")
# testing for various values of min_child_weight
parameters_grid <- expand.grid(nrounds = 80,
max_depth = seq(5, 15, 2),
eta = c(0.25),
gamma = 1,
colsample_bytree = c(0.2),
min_child_weight = seq(200, 1000, 200),
subsample = 0.8)
set.seed(123456789)
model_1 <- train(PTOTVAL ~ .,
data = data_train[selectedvars],
method = "xgbTree", #XGBoost
trControl = control,
tuneGrid  = parameters_grid)
model_1
# testing for various values of colsample_bytree
parameters_grid2 <- expand.grid(nrounds = 80,
max_depth = 9,
eta = c(0.25),
gamma = 1,
colsample_bytree = seq(0.1, 0.8, 0.1),
min_child_weight = 200,
subsample = 0.8)
set.seed(123456789)
model_2 <- train(PTOTVAL ~ .,
data = data_train[selectedvars],
method = "xgbTree",
trControl = control,
tuneGrid  = parameters_grid2)
model_2
# testing for various values of optimal length of the subsample
parameters_grid3 <- expand.grid(nrounds = 80,
max_depth = 9,
eta = c(0.25),
gamma = 1,
colsample_bytree = 0.7,
min_child_weight = 200,
subsample = c(0.6, 0.7, 0.75, 0.8, 0.85, 0.9))
set.seed(123456789)
model_3 <- train(PTOTVAL ~ .,
data = data_train[selectedvars],
method = "xgbTree",
trControl = control,
tuneGrid  = parameters_grid3)
model_3
# lower the learning rate and proportionally increase number of trees
paramters_grid4 <- expand.grid(nrounds = 160, # number of trees increased.
max_depth = 9,
eta = 0.12,  # learnign rate lowered
gamma = 1,
colsample_bytree = 0.7,
min_child_weight = 200,
subsample = 0.9)
set.seed(2234)
model_XGB <- train(PTOTVAL ~ .,
data = data_train[selectedvars],
method = "xgbTree",
trControl = control,
tuneGrid  = paramters_grid4)
model_XGB
prediction_train_XGB <- predict(model_XGB,
data_train[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")
prediction_test_XGB <- predict(model_XGB,
data_test[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")
(stats_XGB <- summary_binary(prediction_test_XGB[,2]))
# How it compares to others
(stats_ALL <- rbind(stats_ALL,stats_XGB))
predictions_train_all <- data.frame(IG = prediction_train_IG_final[,2],
RF = prediction_train_RF[,2],
XG = prediction_train_XGB[,2])
corrplot::corrplot(cor(predictions_train_all), method="number",bg = "yellow") # all predictions are strongly correlated.
train_set <- predictions_train_all
# Averaging (Majority Voting)
predictions_train_Ensemble <- rowMeans(train_set)
# # let's use  generalized linear model (logistic regression).
#
# model_glm<-
# train(train_set[,1:3],train_set[,4],method='glm')
predictions_test_all <- data.frame(IG = prediction_test_IG_final[,2],
RF = prediction_test_RF[,2],
XG = prediction_test_XGB[,2])
# Averaging (Majority Voting)
predictions_test_Ensemble <- rowMeans(predictions_test_all)
(stats_Ensemble <- summary_binary(predictions_test_Ensemble))
(stats_ALL <- rbind(stats_ALL,stats_Ensemble))
data_train_numberic <- data.frame(lapply(data_train,as.numeric))
data_test_numberic <- data.frame(lapply(data_test,as.numeric))
max_points <- apply(data_train_numberic, 2, max)
min_points <- apply(data_train_numberic, 2, min)
data_train.scaled <-
as.data.frame(scale(data_train_numberic,
center = min_points,
scale  = max_points - min_points))
data_test.scaled <-
as.data.frame(scale(data_test_numberic,
center = min_points,
scale  = max_points - min_points))
# training the model
model_NN <- neuralnet(PTOTVAL ~ .,
data = data_train.scaled,
hidden = c(1),
linear.output = T)
# training the model
model_NN <- neuralnet(PTOTVAL ~ .,
data = data_train.scaled,
hidden = c(1),
linear.output = T)
head(data_train)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
requiredPackages = c("e1071","randomForest","caretEnsemble","verification","olsrr","DescTools","caret","tibble","purrr","corrplot","dbplyr","dplyr","readr", "ggplot2", "tidyverse","tree","rpart","rpart.plot","rattle","here", "pROC","xgboost","neuralnet")
for(i in requiredPackages){if(!require(i,character.only = TRUE)) install.packages(i)}
for(i in requiredPackages){if(!require(i,character.only = TRUE)) library(i,character.only = TRUE)}
setwd("C:\\Users\\Tesfahun Boshe\\Documents\\classification-censusincome-ml")
data_train <- read.csv("CensusData_Train.csv")
data_test <- read.csv("CensusData_Test.csv")
str(data_train)
table(data_train$PTOTVAL)
# is.factor(data_train$PTOTVAL)
data_train$PTOTVAL <- factor(data_train$PTOTVAL,
levels = c(" 50000+.","-50000"), #level 5000 is set as the first one
ordered = TRUE) # ordinal
table(data_train$PTOTVAL)
sum(is.na(data_train))
sapply(data_train, function(x) sum(is.na(x)))
sapply(data_train, function(x) sum(x==" ?"))
data_train <- data_train[!data_train$PENATVTY == " ?",]
categorical_vars <-
sapply(data_train, is.character) %>%
which() %>%
names()
categorical_vars
data_train[categorical_vars] <- lapply(data_train[categorical_vars],factor)
lapply(data_train[categorical_vars],levels)
options(contrasts = c("contr.treatment",  # for non-ordinal factors
"contr.treatment")) # for ordinal factors
numeric_vars <-
sapply(data_train, is.numeric) %>%
which() %>%
names()
numeric_vars
sapply(data_train[, numeric_vars],
function(x)
unique(x) %>%
length()) %>%
sort()
correlations <- cor(data_train[, numeric_vars],
use = "pairwise.complete.obs")
# using the 30 most correlated variables
corrplot.mixed(correlations,
upper = "number",
lower = "circle",
tl.col = "black",
tl.pos = "lt")
( findLinearCombos(data_train[,numeric_vars] ) ->
linearCombos )
data_train$PTOTVAL <- ifelse(data_train$PTOTVAL=="-50000",0,1)
categorical_vars_all <- c(categorical_vars,"PTOTVAL") # add the dependent variable to the catergorical variables list.
model <- lm(PTOTVAL ~ .,
data = data_train[categorical_vars_all])
# summary(model)
ols_step_backward_p(model,
prem = 0.05,
# show progress
progress = FALSE) -> backward_selector
backward_selector$removed
categorical_vars_all <- categorical_vars_all[!categorical_vars_all %in% backward_selector$removed]
selectedvars <- c(categorical_vars_all ,numeric_vars)
data_train$PTOTVAL <- as.factor(ifelse(data_train$PTOTVAL==1,"YES","NO"))
data_test <- read.csv("CensusData_Test.csv")
data_test <- data_test[!data_test$PENATVTY == " ?",]
data_test[categorical_vars] <- lapply(data_test[categorical_vars],factor)
data_test$PTOTVAL <- as.factor(ifelse(data_test$PTOTVAL=="-50000","NO","YES"))
data_train_numberic <- data.frame(lapply(data_train,as.numeric))
data_test_numberic <- data.frame(lapply(data_test,as.numeric))
max_points <- apply(data_train_numberic, 2, max)
min_points <- apply(data_train_numberic, 2, min)
data_train.scaled <-
as.data.frame(scale(data_train_numberic,
center = min_points,
scale  = max_points - min_points))
data_test.scaled <-
as.data.frame(scale(data_test_numberic,
center = min_points,
scale  = max_points - min_points))
# training the model
model_NN <- neuralnet(PTOTVAL ~ .,
data = data_train.scaled,
hidden = c(1),
linear.output = T)
# training the model
model_NN <- neuralnet(PTOTVAL ~ .,
data = data_train.scaled,
# hidden = c(1),
linear.output = T)
stats_ALL
(stats_ALL <- rbind(stats_ALL,stats_Ensemble))
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
requiredPackages = c("e1071","randomForest","caretEnsemble","verification","olsrr","DescTools","caret","tibble","purrr","corrplot","dbplyr","dplyr","readr", "ggplot2", "tidyverse","tree","rpart","rpart.plot","rattle","here", "pROC","xgboost","neuralnet")
for(i in requiredPackages){if(!require(i,character.only = TRUE)) install.packages(i)}
for(i in requiredPackages){if(!require(i,character.only = TRUE)) library(i,character.only = TRUE)}
setwd("C:\\Users\\Tesfahun Boshe\\Documents\\classification-censusincome-ml")
data_train <- read.csv("CensusData_Train.csv")
data_test <- read.csv("CensusData_Test.csv")
str(data_train)
table(data_train$PTOTVAL)
# is.factor(data_train$PTOTVAL)
data_train$PTOTVAL <- factor(data_train$PTOTVAL,
levels = c(" 50000+.","-50000"), #level 5000 is set as the first one
ordered = TRUE) # ordinal
table(data_train$PTOTVAL)
sum(is.na(data_train))
sapply(data_train, function(x) sum(is.na(x)))
sapply(data_train, function(x) sum(x==" ?"))
data_train <- data_train[!data_train$PENATVTY == " ?",]
categorical_vars <-
sapply(data_train, is.character) %>%
which() %>%
names()
categorical_vars
data_train[categorical_vars] <- lapply(data_train[categorical_vars],factor)
lapply(data_train[categorical_vars],levels)
options(contrasts = c("contr.treatment",  # for non-ordinal factors
"contr.treatment")) # for ordinal factors
numeric_vars <-
sapply(data_train, is.numeric) %>%
which() %>%
names()
numeric_vars
sapply(data_train[, numeric_vars],
function(x)
unique(x) %>%
length()) %>%
sort()
correlations <- cor(data_train[, numeric_vars],
use = "pairwise.complete.obs")
# using the 30 most correlated variables
corrplot.mixed(correlations,
upper = "number",
lower = "circle",
tl.col = "black",
tl.pos = "lt")
( findLinearCombos(data_train[,numeric_vars] ) ->
linearCombos )
data_train$PTOTVAL <- ifelse(data_train$PTOTVAL=="-50000",0,1)
categorical_vars_all <- c(categorical_vars,"PTOTVAL") # add the dependent variable to the catergorical variables list.
model <- lm(PTOTVAL ~ .,
data = data_train[categorical_vars_all])
# summary(model)
ols_step_backward_p(model,
prem = 0.05,
# show progress
progress = FALSE) -> backward_selector
backward_selector$removed
categorical_vars_all <- categorical_vars_all[!categorical_vars_all %in% backward_selector$removed]
selectedvars <- c(categorical_vars_all ,numeric_vars)
data_train$PTOTVAL <- as.factor(ifelse(data_train$PTOTVAL==1,"YES","NO"))
data_test <- read.csv("CensusData_Test.csv")
data_test <- data_test[!data_test$PENATVTY == " ?",]
data_test[categorical_vars] <- lapply(data_test[categorical_vars],factor)
data_test$PTOTVAL <- as.factor(ifelse(data_test$PTOTVAL=="-50000","NO","YES"))
data_train_numberic <- data.frame(lapply(data_train,as.numeric))
data_test_numberic <- data.frame(lapply(data_test,as.numeric))
max_points <- apply(data_train_numberic, 2, max)
min_points <- apply(data_train_numberic, 2, min)
data_train.scaled <-
as.data.frame(scale(data_train_numberic,
center = min_points,
scale  = max_points - min_points))
data_test.scaled <-
as.data.frame(scale(data_test_numberic,
center = min_points,
scale  = max_points - min_points))
# training the model
model_NN <- neuralnet(PTOTVAL ~ .,
data = data_train.scaled,
hidden = c(1),
linear.output = T)
