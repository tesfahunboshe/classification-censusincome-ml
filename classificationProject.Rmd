 ---
title: "Classification of Census Income"
author: "Tesfahun Tegene Boshe, Cynara Nyahoda"
date: "5/30/2021"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    code_folding: show
    keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```

# Introduction

**The Problem statement**

Classification is a supervised machine learning algorithm of assigning a class label based on known examples. There are many different types of classification tasks requiring machine learning approaches such as those to be discussed below. In this document, we will try to predict a person's income based on census data.

The predictions will be either above 50K or below 50K. Since the dependent variable is of a categorical type, we have a classification problem at hand. 

While there are numerous techniques of classification, we will apply the following three and compare the results:

**1. Classification Trees**

CART  - uses Gini purity/coefficient
C5.0 - uses information gain (IG) - IG is measured by entropy (1-entropy)

Gini coefficient

**2. Random Forests**

Random forest is a supervised learning algorithm/model that uses a large number of ensembled decision trees on data samples. Each tree in the forest makes a prediction and the best solution is selected based on voting. This model is common for its simplicity and diversity hence, the combination of the multiple decision trees is likely to get a more accurate and stable prediction.


**3. Bagging (Bootstrap Averaging**

“predicting with votes”.
Estimating the out-of-bag error - OOB

**a. Subbagging - Subsample Aggregating**


**4. Boosting of decision trees**

“predicting with votes”.
Estimating the out-of-bag error - OOB

**a. GBM: Gradient Boosting Models**

**a.1 Stochastic Gradient Boosting **

**b. XGBost: Extreme Gradient Boosting**
 also called regularized boosting method, has 

- intelligent tree prunning
- Newton-Raphson approximation
- Additional randomization parameter
- parallelization
**c. CatBoost - Category and Boosting**
machine learning algorithm from Yandex - yields state-of-the-art results without extensive data training, and provides powerful out-of-the-box support for the more descriptive data formats. It also handles categorical variables automatically. 

**d. AdaBoost: Adaptive Boosting**

- the new predictor corrects the weights of predecessors were mis-classified or for which we observe string under- or over-prediction. 
 
**5. Ensembling **

combining predictions from various models to improve the result. 

**a. Majority voting or weighted voting **

**b. Stacking - stacked generalization **
- additonal model is trained to conduct aggregation, instead of voting/averaging. Stacking increases accuracy of the model and reduces its variance


**6. Neural Networks **

**Data source**

The source data comes from Census-Income (KDD) Data Set available [Here](https://archive.ics.uci.edu/ml/datasets/Census-Income+%28KDD%29)
The data set is divided into a train and test sets, each with 22 columns, some of which are categorical.

![Data description](./image.jpg)

# Exploratory data analysis
Before we get to the modeling itself, we will analyse the data at hand. 

## Data preparation
Installing the necessary packages 
```{r}

requiredPackages = c("e1071","randomForest","janitor","verification","olsrr","DescTools","caret","tibble","purrr","corrplot","dbplyr","dplyr","readr", "ggplot2", "tidyverse","tree","rpart","rpart.plot","rattle","here", "pROC","xgboost","neuralnet")

for(i in requiredPackages){if(!require(i,character.only = TRUE)) install.packages(i)}
for(i in requiredPackages){if(!require(i,character.only = TRUE)) library(i,character.only = TRUE)}

```  

```{r, echo=FALSE}
setwd("C:\\Users\\48574\\Documents\\Data Science\\Sem3\\Machine Learning 2\\Project")

```

Load the data
```{r}

data_train <- read.csv("CensusData_Train.csv")
data_test <- read.csv("CensusData_Test.csv")
```

Data summary
```{r}
summary(data_train)
```

Let's check our target variable,*PTOTVAL*
```{r}
table(data_train$PTOTVAL)

```

*PTOTVAL* is a ordinal catoegorical variable with two levels (-50000,50000+). 

```{r}
# is.factor(data_train$PTOTVAL)
data_train$PTOTVAL <- factor(data_train$PTOTVAL,
                           levels = c(" 50000+.","-50000"), #level 5000 is set as the first one
                           ordered = TRUE) # ordinal

levels(data_train$PTOTVAL)
table(data_train$PTOTVAL)
```

## Missing values and outliers
Let's find if any entries are missing

```{r}

sum(is.na(data_train))
sapply(data_train, function(x) sum(is.na(x)))
```
> This is not quite true, becasue we know the missing values were represented by a question mark. 

Let's check by variable. 

```{r}
sapply(data_train, function(x) sum(x==" ?"))
```

Variable *PENATVTY* has missing values. 
We will will remove those observations. 

```{r}
data_train <- data_train[!data_train$PENATVTY == " ?",]

```

**Categorical and numeric variables**

a. Categorical variables
```{r}

categorical_vars <- 
  sapply(data_train, is.character) %>% 
  which() %>% 
  names()

categorical_vars

```

Let's change all categorical variables into factors. 
```{r}
data_train[categorical_vars] <- lapply(data_train[categorical_vars],factor)
```

Now we can check the list of levels for each variable. The output is hidden since too long. 
```{r, results = FALSE}
lapply(data_train[categorical_vars],levels)
```

Setting contrast
```{r}
options(contrasts = c("contr.treatment",  # for non-ordinal factors
                      "contr.treatment")) # for ordinal factors
```

a. Numeric variables
```{r}

numeric_vars <- 
  sapply(data_train, is.numeric) %>% 
  which() %>% 
  names()

numeric_vars

```

# Classification methods
In this stage, we will apply the classification techniques discussed in the introduction. Before that, however, we will need to select from the variables.

## Variable selection
Before applying the classification techniques, we will once again go through the variables and select only those that will positively contribute to our model accuracy. Large size of variables can be beneficial to find the true causality relationship, however, it may make the calculations very expensive or sometimes over-fitting may happen. 

## Selecting from numeric variables
Generally, it is advisable to remove variables with a *single level (zero variance)*, variables which are *linear combinations* of each other and variables with *very large correlation* with others. Let's do that one by one.

**1. Removing variables with 1 level**

```{r}

sapply(data_train[, numeric_vars], 
        function(x) 
          unique(x) %>% 
          length()) %>% 
  sort()
```

> No variables with 1 level. 

**2. Removing highly correlated variables**

If we have two highly correlated variables, we remove one from the model. Because they supply redundant information, removing one of the correlated factors usually doesn't drastically reduce the R-squared.
```{r}

correlations <- cor(data_train[, numeric_vars],
    use = "pairwise.complete.obs")


```

Plotting the correlations
```{r}
# using the 30 most correlated variables
corrplot.mixed(correlations,
               upper = "number",
               lower = "circle",
               tl.col = "black",
               tl.pos = "lt")

```

> We see that there are variables *WKSWORK* ( weeks worked in a year) and *AHRSPAY* (wage per hour) have the highest correlation.  We can remove one of them, however, since the correlation is only 75%, we keep both of them 

3. Removing linear combinations

```{r}

( findLinearCombos(data_train[,numeric_vars] ) ->
    linearCombos )
```

> None of the numberic variables is/are in linear combinations with the other/s. 

## Selecting from categorical variables
We can use general regression model to apply logistic regression. Then we will compare the p - values. 

We will need to prepare the dependent variable for this.  
```{r}
data_train$PTOTVAL <- ifelse(data_train$PTOTVAL=="-50000",0,1)
categorical_vars_all <- c(categorical_vars,"PTOTVAL") # add the dependent variable to the catergorical variables list. 

```

```{r}

model <- lm(PTOTVAL ~ ., 
                 data = data_train[categorical_vars_all])

# summary(model)

```
> Each level of a categorical variable is treated as  a separate variable. However, all variables are removed together if insignificant. 

**Automated elimination**

Backward elimination is general to specific approaching, where the model starts with all available variables, and then runs the model again and again, in each step removing 1 least significant variable. The process stops when a model with all variables significant is achieved. Other automated options are step-wise elimination and forward elimination. 

```{r,cache=TRUE}

ols_step_backward_p(model,
                    prem = 0.05,
                    # show progress
                    progress = FALSE) -> backward_selector

```

List of removed variables. 
```{r}

backward_selector$removed
```

> Variables *PRCITSHP*, *AREORGN*,*ARACE* are selected for removal. 

The remaining categorical variables

```{r}
categorical_vars_all <- categorical_vars_all[!categorical_vars_all %in% backward_selector$removed]

```

Let's combine the selected variables. 
```{r}
selectedvars <- c(categorical_vars_all ,numeric_vars)
```

Changing the target variable to a factor of "YES", and "NO". 
```{r}
data_train$PTOTVAL <- as.factor(ifelse(data_train$PTOTVAL==1,"YES","NO"))

```

## Preparing the test data

Let's load the data. 
```{r}
data_test <- read.csv("CensusData_Test.csv")
```


We have to apply all the changes we made to the train data to the test data too. 

1. Remove rows with question mark for variable **
```{r}
data_test <- data_test[!data_test$PENATVTY == " ?",]

```

2. Change all categorical variables to factors.
```{r}
data_test[categorical_vars] <- lapply(data_test[categorical_vars],factor)

```

3. Change the dependent variable to binary levels (0,1)
```{r}
data_test$PTOTVAL <- as.factor(ifelse(data_test$PTOTVAL=="-50000","NO","YES"))



## Decision Trees for classification
```

Let's also define the function for checking the model performance. 

The following function decodes the predicted probability values to binary levels,(0,1), and then calculates the model accuracy statistics from the confusion matrix it creates. 

```{r}
summary_binary <- function(predicted_probs,
                           real,
                           cutoff = 0.5,
                           level_positive = "YES",
                           level_negative = "NO") {

  ctable <- confusionMatrix(as.factor(ifelse(predicted_probs > cutoff, 
                                             level_positive, 
                                             level_negative)), 
                            real, 
                            level_positive) 

  stats <- round(c(ctable$overall[1],
                   ctable$byClass[c(1:4, 7, 11)]),5)
  return(stats)
}
```



### Models
Now that we have selected the variables, we can start modeling. 

*A. Using GINI coefficient as the split criterion *

Let's start with cross-validation for the best results. 
```{r,cache = TRUE}

tc <- trainControl(method = "cv",
                   number = 10, 
                   classProbs = TRUE,
                   summaryFunction = twoClassSummary,
                   savePredictions = "final")

```


```{r}


set.seed(234356) # seeding for reproducibility

model_GINI <- rpart(PTOTVAL ~ ., # model formula
                    data = data_train[selectedvars], # data
                    method = "class", # type of the tree: classification
                    parms = list(split = 'gini'))  # gini for decision criterion


```

Prediction on the train and test data. 
```{r}
prediction_train_GINI <- predict(model_GINI, 
                            data_train[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")

prediction_test_GINI <- predict(model_GINI, 
                            data_test[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")

```

What is we played a little bit with the tree complexity, stopping criteria and prunning parameters?
```{r}

model_GINI_final <- rpart(PTOTVAL ~ ., # model formula
                          data = data_train[selectedvars], # data
                          method = "class", # type of the tree: classification
                          parms = list(split = 'gini'),  # gini for decision criterion
                          minsplit = 500, # chosen after multiple attempts
                          minbucket = 250, # chosen after multiple attempts
                          maxdepth = 30, # chosen after multiple attempts
                          cp = -1
                    
                   )

```

Prediction on the train and test data. 
```{r}
prediction_train_GINI_final <- predict(model_GINI_final, 
                            data_train[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")

prediction_test_GINI_final <- predict(model_GINI_final, 
                            data_test[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")

```

*B. Using Information Gain as the split criterion* 

```{r}

model_IG <- rpart(PTOTVAL ~ ., # model formula
                    data = data_train[selectedvars], # data
                    method = "class", # type of the tree: classification
                    parms = list(split = 'information'))  # entropy for decision criterion


```

Prediction on the train and test data. 
```{r}
prediction_train_IG <- predict(model_GINI_final, 
                            data_train[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")

prediction_test_IG <- predict(model_GINI_final, 
                            data_test[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")

```


With some modification of the parameters: 
```{r}
model_IG_final <- rpart(PTOTVAL ~ ., # model formula
                    data = data_train[selectedvars], # data
                    method = "class", # type of the tree: classification
                    parms = list(split = 'information'),  # entropy for decision criterion
                    minsplit = 500, # chosen after multiple attempts
                    minbucket = 250,# chosen after multiple attempts
                    maxdepth = 30, # chosen after multiple attempts
                    cp = -1 )



```

Prediction on the train and test data. 
```{r}
prediction_train_IG_final <- predict(model_GINI_final, 
                            data_train[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")

prediction_test_IG_pruneed <- predict(model_GINI_final, 
                            data_test[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")
```

### Comparison using Model Plots {.tabset}

#### model_GINI

```{r}

fancyRpartPlot(model_GINI)
```

#### model_GINI_final

```{r}
fancyRpartPlot(model_GINI_final)
```

#### model_IG

```{r}
fancyRpartPlot(model_IG)
```

#### model_IG_final

```{r}
fancyRpartPlot(model_IG_final)
```



## Random Forests


```{r}
# 
# set.seed(1234)
# model_RF <- randomForest(PTOTVAL ~ ., 
#                            data = data_train[selectedvars])
```

Prediction on the train and test data. 
```{r}
# prediction_train_RF <- predict(model_RF, 
#                             data_train[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")
# 
# prediction_test_RF <- predict(model_RF, 
#                             data_test[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")
```

## Bagging (bootstrap averaging)

We will use the best model from the above 4 and apply boosting/bagging algorithms to see if the accuracy will improve. 

We will create 15 models from samples of the data. (Odd to make it easier for majority voting). 
```{r}

n <- nrow(data_train[selectedvars])

results_bagging <- list() # collect resulting models

for (sample in 1:15) {
  # we draw n-element sample (with replacement) 
  set.seed(12345 + sample)
  data_sample <- 
    data_train[selectedvars][sample(x = 1:n, 
                         size = n,
                         replace = TRUE),]

  results_bagging[[sample]] <-  rpart(PTOTVAL ~ .,
                                    data = data_sample, # sample data
                                    method = "class", # type of the tree: classification
                                    parms = list(split = 'information'),  # entropy for decision criterion
                                    minsplit = 500, # chosen value
                                    minbucket = 250,# chosen value
                                    maxdepth = 30, # chosen value
                                    cp = -1 )
  
  rm(data_sample)
}
```

```{r}

prediction_bagging <- 
  sapply(results_bagging,
         function(x) 
           predict(object = x,
                   newdata = data_test[selectedvars[!selectedvars %in% "PTOTVAL"]], # exclude the target variable
                   type = "class")) %>% 
  data.frame()

prediction_bagging <- (prediction_bagging=="YES")*1
```

Majority voting. (If more than half votes for a *YES*, it is a *YES*)
```{r}

prediction_bagging_final <-
  ifelse(rowSums(prediction_bagging) > 15/2,
         "YES", "NO") %>% 
  factor(., levels = c("YES", "NO"))
```


```{r}
confusionMatrix(data = prediction_bagging_final,
                reference =data_test$PTOTVAL,
                positive = "YES")

```

ROC and AUC
```{r}
datausa.ROC.test.bag <-
  roc(as.numeric(data_test$PTOTVAL == "Yes"), 
      prediction_bagging_final2)

```


## Boosting of decision trees
Boosting is a sequentially additive model improvement algorithm, where each subsequent model improves the results of the previous ones. 
There are multiple boosting algorithms. We will use GBM (Gradient Boosting Models) and XGBoost (Extreme Gradient Boosting). 

a. GBM: Gradient Boosting Models

Stochastic Gradient Boosting is a GBM technique with a notion that injecting randomness into function estimation procedures could improve their performance. 
```{r}

# parameter tuning
# parameters_gbm <- expand.grid(interaction.depth = c(1, 2, 4),
#                              n.trees = c(100, 500),
#                              shrinkage = c(0.01, 0.1), 
#                              n.minobsinnode = c(100, 250, 500))
# tc2 <- trainControl(method = "cv", 
#                          number = 3,
#                          classProbs = TRUE,
#                          summaryFunction = twoClassSummary,
#                          savePredictions = "final")



set.seed(123456789)
model_GBM  <- train(PTOTVAL ~ .,
                       data = data_train[selectedvars][1:1000,], # choosing the first 1000 rows from the trainign dataset
                       distribution = "bernoulli",
                       method = "gbm",
                       # tuneGrid = parameters_gbm,
                       # trControl = tc2,
                       verbose = FALSE)

# best set of parameters
model_GBM

# Hence:
# 
# n.trees = 500,
# interaction.depth = 4,
# shrinkage = 0.1
# n.minobsinnode = 100

```

Prediction on the train and test data. 
```{r}
prediction_train_GBM <- predict(model_GBM,
                            data_train[selectedvars[!selectedvars %in% "PTOTVAL"]][1:1000,])

prediction_test_GBM <- predict(model_GBM,
                            data_test[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")
```



b. XGBoost: Extreme Gradient Boosting 


finding optimal values of tree parameters
```{r}

tc2 <- trainControl(method = "cv",
                         number = 3,
                         classProbs = TRUE,
                         summaryFunction = twoClassSummary,
                         savePredictions = "final")


parameters_grid <- expand.grid(nrounds = 80,
                              max_depth = seq(5, 15, 2),
                              eta = c(0.25), 
                              gamma = 1,
                              colsample_bytree = c(0.2),
                              min_child_weight = seq(200, 1000, 200),
                              subsample = 0.8)

set.seed(123456789)
model_1 <- train(PTOTVAL ~ .,
                      data = data_train[selectedvars],
                      method = "xgbTree",
                      trControl = tc2,
                      tuneGrid  = parameters_grid)


model_1

```
Accordingly, max_depth = 9, min_child_weight = 200 

```{r}
#colsample_bytree 
parameters_grid2 <- expand.grid(nrounds = 80,
                              max_depth = 9,
                              eta = c(0.25), 
                              gamma = 1,
                              colsample_bytree = seq(0.1, 0.8, 0.1),
                              min_child_weight = 200,
                              subsample = 0.8)
set.seed(123456789)
model_2 <- train(PTOTVAL ~ .,
                      data = data_train[selectedvars],
                      method = "xgbTree",
                      trControl = tc2,
                      tuneGrid  = parameters_grid2)

model_2

```
Hence,the best value of colsample_bytree = 0.7

Next we find the optimak sample size. 
```{r}

# optimal length of the subsample
parameters_grid3 <- expand.grid(nrounds = 80,
                              max_depth = 9,
                              eta = c(0.25), 
                              gamma = 1,
                              colsample_bytree = 0.7,
                              min_child_weight = 200,
                              subsample = c(0.6, 0.7, 0.75, 0.8, 0.85, 0.9))

set.seed(123456789)
model_3 <- train(PTOTVAL ~ .,
                      data = data_train[selectedvars],
                      method = "xgbTree",
                      trControl = tc2,
                      tuneGrid  = parameters_grid3)

model_3


```
Optimal subsample = 0.9. 
```{r}

# lower the learning rate and proportionally increase number of trees

paramters_grid4 <- expand.grid(nrounds = 160, # number of trees. 
                              max_depth = 9,
                              eta = 0.12,  # learnign rate
                              gamma = 1,
                              colsample_bytree = 0.7,
                              min_child_weight = 200,
                              subsample = 0.9)

set.seed(2234)
model_XGB <- train(PTOTVAL ~ .,
                      data = data_train[selectedvars],
                      method = "xgbTree",
                      trControl = tc2,
                      tuneGrid  = paramters_grid4)

model_XGB
```


Prediction on the train and test data. 
```{r}
prediction_train_XGB <- predict(model_XGB,
                            data_train[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")

prediction_test_XGB <- predict(model_XGB,
                            data_test[selectedvars[!selectedvars %in% "PTOTVAL"]],type = "prob")
```


## Ensembling
Ensembling is the process if useing multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone. 
THis algorithm has the base models as the bottom layer and one final classifier as the top layer model. 

At this stage we will ensemble the models created above. 

```{r}

models <- c("model_GINI_final", "model_IG_final", "model_RF", "model_BGM", "model_GB", "model_XGB")
```

Let's first create a dataframe with the predictions (ofcourse on the train data) from each of them. 
```{r}

predictions_all <- as.data.frame(GINI = prediction_train_GINI_final, 
                                 IG = prediction_train_IGI_final,
                                 RF = prediction_train_RF,
                                 BGM = prediction_train_BGM,
                                 GB = prediction_train_GB,
                                 XG = prediction_train_XG)

```

We can check the correlation between predictions using correlation plots. 
```{r}

corrplot::corrplot(cor(predictions_all)) 

```



Stacking - stacked generalization
- And now we can apply stacked generalization - taking out predictions of success for each observation in the cross-validation process. *caretEnsemble* function from caret package will be used. 


```{r}

# Blending with caretEnsemble

model_ensemble <- caretEnsemble(
  models, 
  metric = "ROC",
  trControl = trainControl(
    number = 2,
    summaryFunction = twoClassSummary,
    classProbs = TRUE,
    savePredictions = "final"
  ))
summary(model_ensemble)

predictions_all$ensemble <- predict(model_ensemble, 
                                newdata = data_test[selectedvars[!selectedvars %in% "PTOTVAL"]], 
                                type = "prob")




```

### Comparing the models with ROC 

```{r}

list(
  datausa.ROC.test.glm      = datausa.ROC.test.glm,
  datausa.ROC.test.rpart    = datausa.ROC.test.rpart,
  datausa.ROC.test.ensemble = datausa.ROC.test.ensemble) %>%
  pROC::ggroc(alpha = 0.5, linetype = 1, size = 1) + 
  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), 
               color = "grey", 
               linetype = "dashed") +
  labs(subtitle = paste0("Gini TEST: ",
                         "glm = ", 
                         round(100 * (2 * auc(datausa.ROC.test.glm) - 1), 3), "%, ",
                         "rpart = ", 
                         round(100 * (2 * auc(datausa.ROC.test.rpart) - 1), 3), "%, ",
                         "ensemble = ", 
                         round(100 * (2 * auc(datausa.ROC.test.ensemble) - 1), 3), "% ")) +
  theme_bw() + coord_fixed() +
  scale_color_brewer(palette = "Set2")

```

```{r}


```

## Neural Networks


All variables need to be rescaled to [0,1] range. We use the training data to calculate the scaling parameters to avoid Look-Ahead Bias. 
```{r}
data_train_numberic <- data.frame(lapply(data_train,as.numeric))
data_test_numberic <- lapply(data_test,as.numeric)

max_points <- apply(data_train_numberic, 2, max)
min_points <- apply(data_train_numberic, 2, min)

data_train.scaled <- 
  as.data.frame(scale(data_train_numberic, 
                      center = min_points, 
                      scale  = max_points - min_points))
data_test.scaled <- 
  as.data.frame(scale(data_test_numberic, 
                      center = min_points, 
                      scale  = max_points - min_points))



```

training the model
```{r}

variables <- names(data_train)

nn <- neuralnet(PTOTVAL ~ ., 
                data   = data_train.scaled,
                # number of neurons in the hidden layer
                hidden = c(1), 
                # T for regression, F for classification
                linear.output = T, 
                threshold = 0.01,
                learningrate.limit = NULL,
                learningrate.factor = list(minus = 0.5, plus = 1.2),
                algorithm = "rprop+")

# the default algorithm is the Resilient Backpropogation (rprop+)
# learningrate.limit sets the upper limit of the *learning rate** coefficient
# learningrate.factor is simply a factor, by which the learning rate coefficient will be changed

plot(nn, rep = "best")
```

Prediction
```{r}
nn.pred <- compute(nn, data_test.scaled[, 1:13])

#opposite transformation (scaling back)
nn.pred.unscaled <- 
  nn.pred$net.result * 
  (max_points["medv"] - min_points["medv"]) + min_points["medv"]


```

```{r}

nn3 <- neuralnet(PTOTVAL ~ ., 
                 data   = data_train.scaled,
                 hidden = c(10, 3), # neurons in the hidden layer
                 linear.output = F, # T for regression, F for classification
                 algorithm = "backprop",
                 learningrate = 0.0001,
                 threshold = 0.1,
                 stepmax = 1e+06,
                 rep = 1)

plot(nn3, rep = "best")
nn3.pred <- compute(nn3, data_test.scaled[, 1:13])

nn3.pred.unscaled <- 
  nn3.pred$net.result * 
  (max_points["medv"] - min_points["medv"]) + min_points["medv"]


```


cross-validation of NNs
```{r}

k <- 10
set.seed(123456789)
index.df <- 
  as.data.frame(split((1:260)[order(runif(260))], 1:10))


set.seed(20200308)
for (i in 1:k) {
  cat("cv fold ", i, "/", k, " ... ", sep = "")
  data_train.cv <- data_train[-index.df[, i],]
  data_test.cv  <- data_train[ index.df[, i],]
  
  # scaling---- ------------------------------
  max_points <- apply(data_train.cv, 2, max) 
  min_points <- apply(data_train.cv, 2, min)
  data_train.cv.scaled <- 
    as.data.frame(scale(data_train.cv, 
                        center = min_points, 
                        scale  = max_points - min_points))
  data_test.cv.scaled <- 
    as.data.frame(scale(data_test.cv, 
                        center = min_points, 
                        scale  = max_points - min_points))
  
  # training the network -----------------------
  nn <- neuralnet(PTOTVAL ~ ., 
                  data = data_train.cv.scaled,
                  hidden = c(5),
                  linear.output = T)
  
  # calculating predictions --------------------
  nn.pred <- compute(nn, data_test.cv.scaled[, 1:13])
  
  # rescaling ----------------------------------
  nn.pred.unscaled <- 
    nn.pred$net.result * 
    (max_points["medv"] - min_points["medv"]) + min_points["medv"]
  
  # calculating the error for a given CV fold 
  (cv.nn.error[i] <- 
      sum((data_test.cv$medv - nn.pred.unscaled)^2) / nrow(data_test.cv))

}


```

Prediction

```{r}

datausa.test.mtx <- 
  model.matrix(object = PTOTVAL ~ ., 
               data = datausa.test)

```

```{r}

datausa.pred.nn <- compute(datausa.nn, datausa.test.mtx[, -1])
datausa.pred.nn$net.result %>% head(10)

```

```{r}

confusionMatrix.nn <-
  confusionMatrix(as.numeric((datausa.pred.nn$net.result > 0.5)) %>% as.factor(),
                  as.factor(ifelse(data_test$PTOTVAL == "Yes", 1, 0)))

```


```

## Prediction

**Decision Trees for classification**

a. CART

```{r}

predict_CART <- predict(model_logit,data_test[selectedvars],
                          type = "response") # type  = "class" to predict directly value of the target variable

```


b. C5.0 

**Random Forests**

**Bagging (bootstrap averaging)**

Subbagging - Subsample Aggregating

**Boosting **

a. GBM: Gradient Boosting Models
a.1 Stochastic Gradient Boosting

b. XGBost: Extreme Gradient Boosting 

```{r}


```

c. CatBoost - Category and Boosting

```{r}


```


d. AdaBoost: Adaptive Boosting

```{r}


```



**Ensembling**


a. Majority voting or weighted voting

b. Stacking - stacked generalization


**Neural Networks**


a.logit
```{r}
predict_logit <- predict(model_logit,data_test[selectedvars],
                          type = "response")
```

b.probit
```{r}
predict_probit <- predict(model_probit,data_test[selectedvars],
                          type = "response")
```


**Naive Bayes**

```{r}
predict_nb <- predict(model_nb,data_test[selectedvars],
                          type = "raw")
predict_nb <- predict_nb[,2]
```

**Random Forest**

Let's first get the train and test data to have the same class. 
```{r}
xtest <- rbind(data_train[1, ] , data_test)
xtest <- xtest[-1,]
```


```{r}
predict_rf <- predict(model_rf,xtest,
                       type = "response")
```


# Evaluation of Performance

We analyse the predictions using the function defined above. Let's first put the predictions all into dataframe. 

```{r}

predictions <- data.frame(logit = predict_logit,probit = predict_probit,
                             nb = predict_nb,rf = predict_rf)

```

The *summary_binary()* requires real values in factor format for *confusionMatrix* calculation. The following code chunk does that. 
```{r}
data_test$PTOTVAL <- factor(data_test$PTOTVAL)
```

Here we call the *summary_binary()* function for every pair of prediction and real value. The results will be compiled into a single dataframe for easier comparison.

```{r}

stats_logit <- summary_binary(predictions$logit,data_test$PTOTVAL) #1. logit
stats_probit <- summary_binary(predictions$probit,data_test$PTOTVAL) #2. probit
stats_nb <- summary_binary(predictions$nb,data_test$PTOTVAL) #3. KNN
stats_rf <- summary_binary(predictions$rf,data_test$PTOTVAL) #4. SVM
```

To compare the results in a table,  
```{r}
metrics <- rbind(stats_logit,stats_probit,stats_nb,stats_rf)
row.names(metrics) <- c("Logit","Probit","Naive Bayes","Random Forest")
metrics
```

## Comparison using ROC curve {.tabset}
An ROC curve plots TPR vs. FPR at different classification thresholds. The area under ROC curve,AUC,can be interpreted as the probability that the model ranks a random positive example more highly than a random negative example. The larger AUC values indicate better models. 

### Logit
```{r}

data_test$PTOTVAL <- ifelse(data_test$PTOTVAL=="0",0,1)
roc.plot(data_test$PTOTVAL,
         predictions$logit)

```

Area under ROC
```{r}
roc.area(data_test$PTOTVAL,
         predictions$logit)$A

```

### Probit
```{r}

roc.plot(data_test$PTOTVAL,
         predictions$probit)

```

Area under ROC
```{r}
roc.area(data_test$PTOTVAL,
         predictions$probit)$A

```


### Naive Bayes
```{r}

roc.plot(data_test$PTOTVAL,
         predictions$nb)

```

Area under ROC
```{r}
roc.area(data_test$PTOTVAL,
         predictions$nb)$A

```

### Random Forest
```{r}

roc.plot(data_test$PTOTVAL,
         predictions$rf)

```

Area under ROC
```{r}
roc.area(data_test$PTOTVAL,
         predictions$rf)$A
```

# Conclusion

After training and running 3 different classification models,  we can depict from the metrics that the Random Forest model has the highest accuracy rate at 94% with the rest at an average of 84%. Naive Bayes however, has the lowest ranking accuracy rate at 83% but is the fastest running model amongst the 3 classifying models used.

Logistic regression models (logit and probit) perform the worst, however, they run fast. The random forest algorithm achieves the highest classification accuracy, but takes the longest time and memory space. Naive Bayes, therefore works the best for this dataset. 

# References
1. https://machinelearningmastery.com/types-of-classification-in-machine-learning/

2. Lecture material